{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, f1_score, make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAR - Missing at random\n",
    "\n",
    "## Im going to only take missings out of two different important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\credit_card_experiments\\encoded_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target feature\n",
    "target_feature = 'card'\n",
    "\n",
    "# Define the percentage of values to make missing\n",
    "missing_percentage = 0.6\n",
    "\n",
    "# Create a mask to introduce missing values, excluding the target feature\n",
    "for column in ['income', 'age']:\n",
    "    mask = np.random.rand(df.shape[0]) < missing_percentage\n",
    "    df.loc[mask, column] = np.nan\n",
    "\n",
    "# Splitting\n",
    "X = df.drop('card', axis=1)\n",
    "y = df['card']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardizing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1055, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     \n",
    "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
    "    'scale_pos_weight' : hp.choice('scale_pos_weight', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10])\n",
    "}\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# optimize f1-score for the minority class (0)\n",
    "scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(seed=0, **params)\n",
    "    score = cross_val_score(estimator=xgboost, \n",
    "                            X=X_train_scaled, \n",
    "                            y=y_train, \n",
    "                            cv=kfold, \n",
    "                            scoring=scorer, \n",
    "                            n_jobs=-1).mean()\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:04<00:00,  9.87trial/s, best loss: -0.9672468056617753]\n",
      "100%|██████████| 64/64 [00:06<00:00, 10.26trial/s, best loss: -0.9672468056617753]\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "best_1 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = Trials())\n",
    "best_2 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 64, trials = Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9734848484848485\n",
      "0.9838709677419355\n",
      "[[ 61   1]\n",
      " [  6 196]]\n",
      "\n",
      "0.9734848484848485\n",
      "0.9838709677419355\n",
      "[[ 61   1]\n",
      " [  6 196]]\n"
     ]
    }
   ],
   "source": [
    "# Train model using the best parameters\n",
    "xgboost_1 = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best_1)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best_1)['gamma'], \n",
    "                           learning_rate=space_eval(space, best_1)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best_1)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best_1)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best_1)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best_1)['scale_pos_weight']\n",
    "                           ).fit(X_train_scaled,y_train)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_1.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_1.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)\n",
    "print()\n",
    "\n",
    "xgboost_2 = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best_2)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best_2)['gamma'], \n",
    "                           learning_rate=space_eval(space, best_2)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best_2)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best_2)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best_2)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best_2)['scale_pos_weight']\n",
    "                           ).fit(X_train_scaled,y_train)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_1.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_1.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the columns all together\n",
    "\n",
    "X_train_scaled_removed = np.delete(X_train_scaled, [1,2], axis=1)\n",
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     \n",
    "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
    "    'scale_pos_weight' : hp.choice('scale_pos_weight', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10])\n",
    "}\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# optimize f1-score for the minority class (0)\n",
    "scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(seed=0, **params)\n",
    "    score = cross_val_score(estimator=xgboost, \n",
    "                            X=X_train_scaled_removed, \n",
    "                            y=y_train, \n",
    "                            cv=kfold, \n",
    "                            scoring=scorer, \n",
    "                            n_jobs=-1).mean()\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:22<00:00,  1.45trial/s, best loss: -0.9672468056617753]\n",
      "100%|██████████| 64/64 [00:05<00:00, 12.06trial/s, best loss: -0.9672468056617753]\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "best_3 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 32, trials = Trials())\n",
    "best_4 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 64, trials = Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9734848484848485\n",
      "0.9838709677419355\n",
      "[[ 61   1]\n",
      " [  6 196]]\n"
     ]
    }
   ],
   "source": [
    "# Train model using the best parameters\n",
    "xgboost_2 = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best_3)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best_3)['gamma'], \n",
    "                           learning_rate=space_eval(space, best_3)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best_3)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best_3)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best_3)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best_3)['scale_pos_weight']\n",
    "                           ).fit(X_train_scaled_removed,y_train)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_1.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_1.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
