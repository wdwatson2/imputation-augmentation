{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, f1_score, make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from hyperopt import tpe, STATUS_OK, Trials, hp, fmin, STATUS_OK, space_eval\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCAR - Missing completely at random\n",
    "## 70% of values are taken out at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>card</th>\n",
       "      <th>reports</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>share</th>\n",
       "      <th>expenditure</th>\n",
       "      <th>owner</th>\n",
       "      <th>selfemp</th>\n",
       "      <th>dependents</th>\n",
       "      <th>months</th>\n",
       "      <th>majorcards</th>\n",
       "      <th>active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37.66667</td>\n",
       "      <td>4.5200</td>\n",
       "      <td>0.033270</td>\n",
       "      <td>124.983300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.25000</td>\n",
       "      <td>2.4200</td>\n",
       "      <td>0.005217</td>\n",
       "      <td>9.854167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.66667</td>\n",
       "      <td>4.5000</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.50000</td>\n",
       "      <td>2.5400</td>\n",
       "      <td>0.065214</td>\n",
       "      <td>137.869200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32.16667</td>\n",
       "      <td>9.7867</td>\n",
       "      <td>0.067051</td>\n",
       "      <td>546.503300</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   card  reports       age  income     share  expenditure  owner  selfemp  \\\n",
       "0     1        0  37.66667  4.5200  0.033270   124.983300      1        0   \n",
       "1     1        0  33.25000  2.4200  0.005217     9.854167      0        0   \n",
       "2     1        0  33.66667  4.5000  0.004156    15.000000      1        0   \n",
       "3     1        0  30.50000  2.5400  0.065214   137.869200      0        0   \n",
       "4     1        0  32.16667  9.7867  0.067051   546.503300      1        0   \n",
       "\n",
       "   dependents  months  majorcards  active  \n",
       "0           3      54           1      12  \n",
       "1           3      34           1      13  \n",
       "2           4      58           1       5  \n",
       "3           0      25           1       7  \n",
       "4           2      64           1       5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'..\\credit_card_experiments\\encoded_dataset.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target feature\n",
    "target_feature = 'card'\n",
    "\n",
    "# Define the percentage of values to make missing\n",
    "missing_percentage = 0.7\n",
    "\n",
    "# Create a mask to introduce missing values, excluding the target feature\n",
    "for column in df.columns:\n",
    "    if column != target_feature:\n",
    "        mask = np.random.rand(df.shape[0]) < missing_percentage\n",
    "        df.loc[mask, column] = np.nan\n",
    "\n",
    "# Splitting\n",
    "X = df.drop('card', axis=1)\n",
    "y = df['card']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardizing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     \n",
    "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
    "    'scale_pos_weight' : hp.choice('scale_pos_weight', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10])\n",
    "}\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# optimize f1-score for the minority class (0)\n",
    "scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(seed=0, **params)\n",
    "    score = cross_val_score(estimator=xgboost, \n",
    "                            X=X_train_scaled, \n",
    "                            y=y_train, \n",
    "                            cv=kfold, \n",
    "                            scoring=scorer, \n",
    "                            n_jobs=-1).mean()\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:23<00:00,  2.04trial/s, best loss: -0.40651464385603364]\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42803030303030304\n",
      "0.9032258064516129\n",
      "[[ 56   6]\n",
      " [145  57]]\n"
     ]
    }
   ],
   "source": [
    "#first model trained for f1\n",
    "# Train model using the best parameters\n",
    "xgboost_1 = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best)['gamma'], \n",
    "                           learning_rate=space_eval(space, best)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best)['scale_pos_weight']\n",
    "                           ).fit(X_train_scaled,y_train)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_1.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_1.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create flag for missing values\n",
    "1. Impute missing values \n",
    "1. Generate samples for the minority class until balanced using SMOTE\n",
    "1. Use flag column to delete values that should be missing\n",
    "1. Delete flag columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['card', 'reports', 'age', 'income', 'share', 'expenditure', 'owner',\n",
      "       'selfemp', 'dependents', 'months', 'majorcards', 'active'],\n",
      "      dtype='object')\n",
      "['reports_missing_flag', 'age_missing_flag', 'income_missing_flag', 'share_missing_flag', 'expenditure_missing_flag', 'owner_missing_flag', 'selfemp_missing_flag', 'dependents_missing_flag', 'months_missing_flag', 'majorcards_missing_flag', 'active_missing_flag']\n"
     ]
    }
   ],
   "source": [
    "#Flags\n",
    "flag_col = []\n",
    "\n",
    "# Create a copy of the original DataFrame\n",
    "df_with_flags = df.copy()\n",
    "\n",
    "print(df_with_flags.columns)\n",
    "\n",
    "# Iterate through each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Create a new flag column for each feature\n",
    "    flag_column = f'{column}_missing_flag'\n",
    "    flag_col.append(flag_column)\n",
    "    \n",
    "    # Set the flag column to 1 where the original column has missing values (NaN), 0 otherwise\n",
    "    df_with_flags[flag_column] = df[column].isna().astype(int)\n",
    "\n",
    "df_with_flags = df_with_flags.drop('card_missing_flag', axis=1)\n",
    "flag_col.remove('card_missing_flag')\n",
    "\n",
    "df_with_flags.head()\n",
    "print(flag_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "card                        False\n",
       "reports                     False\n",
       "age                         False\n",
       "income                      False\n",
       "share                       False\n",
       "expenditure                 False\n",
       "owner                       False\n",
       "selfemp                     False\n",
       "dependents                  False\n",
       "months                      False\n",
       "majorcards                  False\n",
       "active                      False\n",
       "reports_missing_flag        False\n",
       "age_missing_flag            False\n",
       "income_missing_flag         False\n",
       "share_missing_flag          False\n",
       "expenditure_missing_flag    False\n",
       "owner_missing_flag          False\n",
       "selfemp_missing_flag        False\n",
       "dependents_missing_flag     False\n",
       "months_missing_flag         False\n",
       "majorcards_missing_flag     False\n",
       "active_missing_flag         False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seperate categorical and continuous features because they need to be imputed differently\n",
    "\n",
    "cat_features = [\n",
    "    'owner',\n",
    "    'selfemp',\n",
    "    'dependents',\n",
    "    'reports',\n",
    "    'majorcards'\n",
    "]\n",
    "\n",
    "cont_features = [\n",
    "    'age',\n",
    "    'income',\n",
    "    'share',\n",
    "    'expenditure',\n",
    "    'months',\n",
    "    'active'\n",
    "]\n",
    "\n",
    "features = cat_features + cont_features\n",
    "\n",
    "#cat featutes first\n",
    "for col in cat_features:\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df_with_flags[col] = cat_imputer.fit_transform(df_with_flags[col].values.reshape(-1,1))[:,0]\n",
    "    \n",
    "#cont features\n",
    "for col in cont_features:\n",
    "    cont_imputer = SimpleImputer(strategy='mean')\n",
    "    df_with_flags[col] = cont_imputer.fit_transform(df_with_flags[col].values.reshape(-1,1))[:,0]\n",
    "\n",
    "\n",
    "df_with_flags.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "X = df_with_flags.drop('card', axis=1)\n",
    "y = df_with_flags['card']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset samples per class Counter({1: 821, 0: 234})\n",
      "reports\n",
      "owner\n",
      "selfemp\n",
      "dependents\n",
      "majorcards\n",
      "reports_missing_flag\n",
      "age_missing_flag\n",
      "income_missing_flag\n",
      "share_missing_flag\n",
      "expenditure_missing_flag\n",
      "owner_missing_flag\n",
      "selfemp_missing_flag\n",
      "dependents_missing_flag\n",
      "months_missing_flag\n",
      "majorcards_missing_flag\n",
      "active_missing_flag\n",
      "[0, 5, 6, 7, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "Resampled dataset samples per class Counter({1: 821, 0: 821})\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic samples for minority class\n",
    "print(f'Original dataset samples per class {Counter(y_train)}')\n",
    "\n",
    "cat_features_idx = []\n",
    "#get the indexes of the cat columns\n",
    "for idx in range(len(X.columns)):\n",
    "    if(X.columns[idx] in cat_features or X.columns[idx] in flag_col):\n",
    "        cat_features_idx.append(idx)\n",
    "        print(X.columns[idx])\n",
    "\n",
    "print(cat_features_idx)\n",
    "\n",
    "sm = SMOTENC(random_state=42, categorical_features=cat_features_idx)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(f'Resampled dataset samples per class {Counter(y_train_res)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize\n",
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data for the selected features\n",
    "X_train_scaled_features = scaler.fit_transform(X_train_res[features])\n",
    "\n",
    "# Transform the test data using the same scaler for the selected features\n",
    "X_test_scaled_features = scaler.transform(X_test[features])\n",
    "\n",
    "# Get the flag columns as NumPy arrays\n",
    "X_train_flags = X_train_res[flag_col].values\n",
    "X_test_flags = X_test[flag_col].values\n",
    "\n",
    "# Combine the scaled features and flag columns using np.concatenate\n",
    "X_train_scaled = np.concatenate((X_train_scaled_features, X_train_flags), axis=1)\n",
    "X_test_scaled = np.concatenate((X_test_scaled_features, X_test_flags), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:14<00:00,  3.39trial/s, best loss: -0.7191184147919875]\n"
     ]
    }
   ],
   "source": [
    "#lets see if keeping flag column and imputed values is good for the model\n",
    "\n",
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     \n",
    "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
    "    'scale_pos_weight' : hp.choice('scale_pos_weight', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10])\n",
    "}\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# optimize f1-score for the minority class (0)\n",
    "scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(seed=0, **params)\n",
    "    score = cross_val_score(estimator=xgboost, \n",
    "                            X=X_train_scaled, \n",
    "                            y=y_train_res, \n",
    "                            cv=kfold, \n",
    "                            scoring=scorer, \n",
    "                            n_jobs=-1).mean()\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# Optimize\n",
    "best_2 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4621212121212121\n",
      "0.8870967741935484\n",
      "[[ 55   7]\n",
      " [135  67]]\n"
     ]
    }
   ],
   "source": [
    "xgboost_with_flags = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best_2)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best_2)['gamma'], \n",
    "                           learning_rate=space_eval(space, best_2)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best_2)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best_2)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best_2)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best_2)['scale_pos_weight']\n",
    "                           ).fit(X_train_scaled,y_train_res)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_with_flags.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_with_flags.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out flags and values that flag indicate\n",
    "# Iterate over flag columns and update corresponding feature values\n",
    "\n",
    "flag_columns = [col for col in X_train_res.columns if col.endswith('_missing_flag')]\n",
    "\n",
    "for flag_col in flag_columns:\n",
    "    feature_col = flag_col.replace('_missing_flag', '')\n",
    "    X_train_res.loc[X_train_res[flag_col] == 1, feature_col] = np.nan\n",
    "    X_test.loc[X_test[flag_col] == 1, feature_col] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res.drop(columns=flag_columns, inplace=True)\n",
    "X_test.drop(columns=flag_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the training data for the selected features\n",
    "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
    "\n",
    "# Transform the test data using the same scaler for the selected features\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space\n",
    "space = {\n",
    "    'learning_rate': hp.choice('learning_rate', [0.0001,0.001, 0.01, 0.1, 1]),\n",
    "    'max_depth' : hp.choice('max_depth', range(3,21,3)),\n",
    "    'gamma' : hp.choice('gamma', [i/10.0 for i in range(0,5)]),\n",
    "    'colsample_bytree' : hp.choice('colsample_bytree', [i/10.0 for i in range(3,10)]),     \n",
    "    'reg_alpha' : hp.choice('reg_alpha', [1e-5, 1e-2, 0.1, 1, 10, 100]), \n",
    "    'reg_lambda' : hp.choice('reg_lambda', [1e-5, 1e-2, 0.1, 1, 10, 100]),\n",
    "    'scale_pos_weight' : hp.choice('scale_pos_weight', [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10])\n",
    "}\n",
    "\n",
    "# Set up the k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "# optimize f1-score for the minority class (0)\n",
    "scorer = make_scorer(f1_score, pos_label = 0)\n",
    "\n",
    "# Objective function\n",
    "def objective(params):\n",
    "    \n",
    "    xgboost = xgb.XGBClassifier(seed=0, **params)\n",
    "    score = cross_val_score(estimator=xgboost, \n",
    "                            X=X_train_res_scaled, \n",
    "                            y=y_train_res, \n",
    "                            cv=kfold, \n",
    "                            scoring=scorer, \n",
    "                            n_jobs=-1).mean()\n",
    "    # Loss is negative score\n",
    "    loss = - score\n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.43trial/s, best loss: -0.7210730786319258]\n"
     ]
    }
   ],
   "source": [
    "# Optimize\n",
    "best_3 = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = 48, trials = Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4393939393939394\n",
      "0.8709677419354839\n",
      "[[ 54   8]\n",
      " [140  62]]\n"
     ]
    }
   ],
   "source": [
    "xgboost_1 = xgb.XGBClassifier(seed=0, \n",
    "                           colsample_bytree=space_eval(space, best_3)['colsample_bytree'], \n",
    "                           gamma=space_eval(space, best_3)['gamma'], \n",
    "                           learning_rate=space_eval(space, best_3)['learning_rate'], \n",
    "                           max_depth=space_eval(space, best_3)['max_depth'], \n",
    "                           reg_alpha=space_eval(space, best_3)['reg_alpha'],\n",
    "                           reg_lambda=space_eval(space, best_3)['reg_lambda'],\n",
    "                           scale_pos_weight=space_eval(space, best_3)['scale_pos_weight']\n",
    "                           ).fit(X_train_res_scaled,y_train_res)\n",
    "# Make prediction using the best model\n",
    "bayesian_opt_predict = xgboost_1.predict(X_test_scaled)\n",
    "# Get predicted probabilities\n",
    "bayesian_opt_predict_prob = xgboost_1.predict_proba(X_test_scaled)[:,1]\n",
    "# Get performance metrics\n",
    "acc = accuracy_score(y_test, bayesian_opt_predict)\n",
    "recall = recall_score(y_test, bayesian_opt_predict, pos_label=0)\n",
    "conf = confusion_matrix(y_test, bayesian_opt_predict)\n",
    "\n",
    "\n",
    "print(acc)\n",
    "print(recall)\n",
    "print(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
